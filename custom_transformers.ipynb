{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "custom transformers.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNPX6nxom31z5ZckxPK5wjB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krumeto/categorical_stuff/blob/main/custom_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOgEG_0xrzHX"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Msb5WMzHsN3P"
      },
      "source": [
        "class CollectionToStringTransformer:\n",
        "  \n",
        "  \"\"\"A class to encode features with multiple string entries into one single string\n",
        "  \n",
        "  df_orig is the original dataframe\n",
        "  index_col: string,  is the column to group the df by\n",
        "  col_to_transform a string or a list, columns to combine into a string\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, df_orig):\n",
        "      self.df = df_orig.copy()\n",
        "      \n",
        "  def get_collection_per_index(self, index_col, col_to_transform):\n",
        "    if isinstance(col_to_transform, str):\n",
        "      mapping = self.df.groupby([index_col])[col_to_transform].apply(set).reset_index(name=f'{col_to_transform}_set')\n",
        "      mapping[f'{col_to_transform}_set'] = mapping[f'{col_to_transform}_set'].apply(sorted)\n",
        "    \n",
        "    if isinstance(col_to_transform, list):\n",
        "      mapping = self.df.groupby([index_col])[col_to_transform].agg(set)\n",
        "      mapping.columns = [f'{col}_set' for col in mapping.columns]\n",
        "      \n",
        "      for col in mapping.columns:\n",
        "        mapping[col] = mapping[col].apply(sorted)\n",
        "        \n",
        "      mapping = mapping.reset_index()\n",
        "      \n",
        "    return mapping\n",
        "    \n",
        "  def transform(self, index_col, col_to_transform, drop_set = True):\n",
        "    \n",
        "    if isinstance(col_to_transform, str):\n",
        "      transformed = self.get_collection_per_index(index_col, col_to_transform)\n",
        "      transformed[f\"{col_to_transform}_string\"] = transformed[f'{col_to_transform}_set'].apply(lambda x: ' '.join([str(i) for i in x]))\n",
        "      \n",
        "      if drop_set:\n",
        "        transformed = transformed.drop([f'{col_to_transform}_set'], axis=1)\n",
        "    \n",
        "    \n",
        "    if isinstance(col_to_transform, list):\n",
        "      transformed = self.get_collection_per_index(index_col, col_to_transform)\n",
        "      for col in col_to_transform:\n",
        "        transformed[f\"{col}_string\"] = transformed[f'{col}_set'].apply(lambda x: ' '.join([str(i) for i in x]))\n",
        "        \n",
        "        if drop_set:\n",
        "          transformed = transformed.drop([f'{col}_set'], axis=1)\n",
        "        \n",
        "    return transformed\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Nq1jwoMsQ3y"
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import HashingVectorizer, CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "\n",
        "class HashTfidfSvdTransformer(TransformerMixin, BaseEstimator):\n",
        "  \"\"\"A class to encode high cardinality categorical variables into a\n",
        "  TfIdf Matrix.\n",
        "  n_components: the number of max_features for the CountVectorizer\n",
        "  hashing_kwargs: parameters to use for the CountVectorizer\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, n_components, hashing_kwargs):\n",
        "    self.n_components = n_components\n",
        "    self.hashing_kwargs = hashing_kwargs\n",
        "    \n",
        "  \n",
        "  def fit(self, df_orig, col_orig):\n",
        "    self.df = df_orig.copy()\n",
        "    \n",
        "    self.hasher = CountVectorizer(\n",
        "      max_features=self.n_components,\n",
        "      **self.hashing_kwargs)\n",
        "    \n",
        "    self.vectorizer = make_pipeline(self.hasher, TfidfTransformer())\n",
        "    self.vectorizer.fit(self.df[col_orig])\n",
        "\n",
        "    return self\n",
        "  \n",
        "  def transform(self,X, col_to_encode):\n",
        "    check_is_fitted(self, ['df', \"n_components\", \n",
        "                       #    'vectorizer', 'sparce_matrix',\n",
        "                       #   'svd', 'regr'\n",
        "                       ])\n",
        "    self.dataset = X[col_to_encode]\n",
        "    \n",
        "    self.sparse_matrix = self.vectorizer.transform(self.dataset).toarray()\n",
        "    print(self.sparse_matrix.shape)\n",
        "    print(self.n_components)\n",
        "  \n",
        "#    self.svd = TruncatedSVD(n_components=self.n_components, random_state=42)\n",
        "#    self.regr = self.svd.fit_transform(self.sparse_matrix)\n",
        "    \n",
        "    \n",
        "    col_names = [f\"{col_to_encode}_diag_component_{i}\" for i in range(self.sparse_matrix.shape[1])]\n",
        "    for i, name in enumerate(col_names):\n",
        "      X[name] = self.sparse_matrix[:,i]\n",
        "      \n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxMNT5DMsvh2"
      },
      "source": [
        "class SubstringOnehotEncoder(TransformerMixin, BaseEstimator):\n",
        "  \"\"\"A class to encode just a subset of top codes as one hots. The class is able to\n",
        "  search for a match within a string.\n",
        "  \n",
        "  For example if a list_of_strings is ['Boston', 'Chicago'] both 'Boston Celtics' and\n",
        "  'Chicago Bulls' are going to be encoded.\n",
        "  \n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, drop_original = False, method='onehot', n_components = None):\n",
        "    self.drop_original = drop_original\n",
        "    self.method = method\n",
        "    self.n_components = n_components\n",
        "    \n",
        "    if self.n_components is None and self.method == 'nmf':\n",
        "      raise ValueError(\"Please define n_components for method nmf\")\n",
        "    \n",
        "  def fit(self, df):\n",
        "    self.df = df.copy()\n",
        "    return self\n",
        "\n",
        "  def transform(self, column , list_of_strings):\n",
        "    allowed_methods = ['onehot', 'nmf']\n",
        "    if self.method not in allowed_methods:\n",
        "      raise ValueError(f\"Please choose one of the following methods: {allowed_methods}\")\n",
        "    \n",
        "    \n",
        "    \n",
        "    if self.method == 'onehot':\n",
        "      check_is_fitted(self, ['df'])\n",
        "      transformed = self.df\n",
        "      \n",
        "      for substring in list_of_strings:\n",
        "        transformed[f\"{column}_{substring}\"] = transformed[column].str.contains(str(substring), case=False, na=0).astype('Int64')\n",
        "    \n",
        "    if self.method == 'nmf':\n",
        "      print('NMF method does not retain state. Please beware in production.')\n",
        "      check_is_fitted(self, ['df', 'n_components'])\n",
        "\n",
        "      codes_df = pd.DataFrame()\n",
        "      transformed = self.df\n",
        "      \n",
        "      for substring in list_of_strings:\n",
        "        codes_df[f\"{substring}\"] = self.df[column].str.contains(str(substring), case=False, na=0).astype('Int64')\n",
        "          \n",
        "      print('Starting NMF')\n",
        "      nmf = NMF(n_components=self.n_components)\n",
        "      W = nmf.fit_transform(codes_df)\n",
        "  \n",
        "      col_names = [f\"{column}_component_{i}\" for i in range(self.n_components)]\n",
        "      for i, name in enumerate(col_names):\n",
        "        transformed[name] = W[:,i]\n",
        "\n",
        "\n",
        "    \n",
        "    if self.drop_original:\n",
        "      transformed = transformed.drop(column, axis=1)\n",
        "      \n",
        "    return transformed\n",
        "  \n",
        "  \n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}